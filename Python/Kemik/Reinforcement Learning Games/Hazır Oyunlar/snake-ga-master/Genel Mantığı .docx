    def network(self, weights=None):
        model = Sequential()  # sequential bir sinir ağı oluşturduk.
        model.add(Dense(output_dim=120, activation='relu', input_dim=11)) 11 tane input layerımıza özellik koyduk.#Aktivasyon olarak neden reluyu seçtiğimizi asağıda anlattık.
        model.add(Dropout(0.15))
        model.add(Dense(output_dim=120, activation='relu'))
        model.add(Dropout(0.15))
        model.add(Dense(output_dim=120, activation='relu'))
        model.add(Dropout(0.15))
        model.add(Dense(output_dim=3, activation='softmax')) # Son kısım softmax olmalı verileri 0 ile 1 arasına yerleştirir.
        opt = Adam(self.learning_rate) # optimizasyon ekliyoruz
        model.compile(loss='mse', optimizer=opt)

        if weights:
            model.load_weights(weights)
        return model
        Aktivasyon Fonksiyonu
Aktivasyon fonksiyonları modele non-linearite katıyor. Gizli katmanlarda (Hidden layer’da) y = f(x,w) şeklindeki lineer fonksiyonumuzda matris çarpımı yapılıp nöronların ağırlığı hesaplandıktan sonra çıktı doğrusal olmayan (non-linear) bir değere dönüştürülür. Çünkü derin öğrenme yöntemleri doğrusal olmayan (non-linear) yapıya sahip problemlerin çözümünde diğer yöntemlere göre daha etkili olduğu için, derin öğrenme yöntemleriyle çözülmeye çalışılan problem genelde doğrusal olmayan non-linear bir problemdir. Matris çarpımı sonucu elde edilen değerin non-linear hale dönüştürülmesi aktivasyon fonksiyonları ile yapılmaktatır. Aktivasyon fonksiyonları çok katmanlı yapay sinir ağlarında doğrusal olmayan (non-linear) dönüşüm işlemleri için kullanılmaktadır. Gizli katmanlarda (Hidden layer’larda) geri türev alınabilmesi (gradient decent hesaplanabilmesi) için (öğrenmede fark geri türevle alınıyor) gizli katmanların (hidden layer) çıktısı bazı aktivasyon fonksiyonları ile normalize edilmektedir. Bu aktivasyon fonksiyonlarından bazıları Şekil 3'de fonksiyon grafikleri verilen sigmoid, tanch, ReLu, PreLu vb’dir. İçlerinde en kullanışlısı ReLu’dur. ReLu’da sigmoid’e göre parametreler daha hızlı bir şekilde öğrenilmektedir. PReLu ise, ReLU’nun kaçırdığı negatif değerleri yakalamaktadır; eğer bizim için negatif değerler önemliyse PReLu tercih edilmelidir.

Aktivasyon fonksiyonlarını gradient descent ile geri dönüp düzeltme yaptığımızda da kullanmaktayız. Bu tarz kullanımdaki amacımız kolay türev alabilmektir.


Şekil 3: En bilinen aktivasyon fonksiyonlarının grafikleri
Aktivasyon fonksiyonlarının bazı özellikleri aşağıdaki gibidir;

İleri beslemeli (Feedforward) ağlarda genelde ReLu versiyonları kullanılmaktadır.
PeRelu, ELU, Maxout’da kullanılması tavsiye edilen diğer aktivasyon fonksiyonlarıdır.
Tanjant hiperbolik de kullanılabilir fakat çok fazla bir şey beklememek gerekiyor.
Sigmoid kullanılması ise genelde tavsiye edilmiyor.
Tanjant hiberbolik sigmoid göre tercih ediliyor.
Google tarafından son dönemde ortaya atılan swish fonksiyonu da çok küçükte olsa başarımı bir miktar artırdığı belirtilmektedir.
Swish, ReLu’ya göre %20 daha yavaş çalışmaktadır. Bununla birlikte 0.001'lik oranında daha başarılı sonuçlar vermektedir.
Epocs ->>> Verinin kaç defa taranacağı
batch_Size -> Bit genişliği 
